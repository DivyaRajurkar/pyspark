{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bafe4c2e-3045-40fe-86dc-3e096e5c25a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\bigdata\\anaconda3\\lib\\site-packages (3.5.1)\nRequirement already satisfied: py4j==0.10.9.7 in c:\\bigdata\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\nNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a74c36-d247-483c-a024-54f6a1d55a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(1, 'divya')>\n"
     ]
    }
   ],
   "source": [
    "# Import the Row class from pyspark.sql\n",
    "# Row is used to represent a single row of data in PySpark DataFrames\n",
    "from pyspark.sql import Row  \n",
    "\n",
    "# Create a Row object with two values: 1 and 'divya'\n",
    "# Since no field names are provided, PySpark assigns default field names: _1 and _2\n",
    "r1 = Row(1, 'divya')  \n",
    "\n",
    "# Print the Row object\n",
    "# Output will display as Row(_1=1, _2='divya') because default field names are used\n",
    "print(r1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c5b6c6-ef3d-4f13-81bc-1f50b143cbbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divya\n"
     ]
    }
   ],
   "source": [
    "# The print() function outputs the specified message to the console.\n",
    "# In this case, it will print the string 'divya'.\n",
    "print('divya')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a94dac-7aa6-4776-89ef-092484ac508f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "# The type() function returns the type of the specified object.\n",
    "# In this case, it will return the data type of the variable 'r1'.\n",
    "# If 'r1' is a Row object (from pyspark.sql), the output will be <class 'pyspark.sql.types.Row'>.\n",
    "print(type(r1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda5c9a9-2c1e-45c6-b478-0b74bf162dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: Row(id=1, name='sanket')"
     ]
    }
   ],
   "source": [
    "# Import the Row class from pyspark.sql\n",
    "# Row is used to represent a single row of data in PySpark, similar to a record in a DataFrame\n",
    "from pyspark.sql import Row  \n",
    "\n",
    "# Create a Row object with two fields: 'id' and 'name'\n",
    "# 'id' is assigned the value 1, and 'name' is assigned the value 'sanket'\n",
    "r1 = Row(id=1, name=\"sanket\")  \n",
    "\n",
    "# Display the Row object\n",
    "# In an interactive environment like databricks,Jupyter Notebook, simply typing 'r1' displays its contents\n",
    "# The output will show: Row(id=1, name='sanket')\n",
    "r1  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365631e9-fb40-47ba-89b9-582398c147d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: (1, 'sanket')"
     ]
    }
   ],
   "source": [
    "r1['id'], r1['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0445ae0-a3a5-4104-9d6d-9ab73ef86c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|Sanket|\n|  2| Akash|\n|  3|Rajesh|\n|  4| divya|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "The code provided works in Databricks because Databricks automatically initializes a SparkSession object called spark. However, in Jupyter Notebook, you need to explicitly create the SparkSession.\n",
    "'''\n",
    "\n",
    "# Import all classes and functions from pyspark.sql module\n",
    "# This includes SparkSession, Row, DataFrame, and various SQL functions\n",
    "from pyspark.sql import *  \n",
    "\n",
    "# Create Row objects representing individual records with 'id' and 'name' fields\n",
    "r1 = Row(id=1, name='Sanket')   # Row with id=1 and name='Sanket'\n",
    "r2 = Row(id=2, name='Akash')    # Row with id=2 and name='Akash'\n",
    "r3 = Row(id=3, name='Rajesh')   # Row with id=3 and name='Rajesh'\n",
    "r4 = Row(id=4, name='divya')    # Row with id=4 and name='divya'\n",
    "\n",
    "# Combine all Row objects into a list (this will be used to create the DataFrame)\n",
    "data1 = [r1, r2, r3, r4]  \n",
    "\n",
    "# Create a DataFrame from the list of Row objects using the SparkSession object 'spark'\n",
    "# Note: In Databricks, 'spark' is available by default. In Jupyter Notebook, you must create it using SparkSession.builder.\n",
    "df = spark.createDataFrame(data1)  \n",
    "\n",
    "# Display the contents of the DataFrame in a tabular format\n",
    "df.show()  \n",
    "\n",
    "# Print the schema of the DataFrame to show column names and data types\n",
    "df.printSchema()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff048c3e-eb2c-40ee-9449-2872c625650b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Both codes create a DataFrame using `Row` in PySpark, but the difference is:  \n",
    "\n",
    "- **Code 1:(above code)** Defines each row with named fields (`id`, `name`) directly.  \n",
    "- **Code 2:(below code)** Uses a **schema-based Row** (`student = Row('id','Name')`) to define column names, providing a reusable structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e6333a-9a76-4ea9-8b91-efdf2427eab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  Name|\n+---+------+\n|  1|Sanket|\n|  2| Akash|\n|  3|Rajesh|\n|  4| divya|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- Name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Import the Row class from pyspark.sql\n",
    "# Row is used to represent a record with fields that can later be converted into a DataFrame\n",
    "from pyspark.sql import Row  \n",
    "\n",
    "# Define a Row class with the specified column names: 'id' and 'Name'\n",
    "# This will act as the schema when creating the DataFrame\n",
    "student = Row('id', 'Name')  \n",
    "\n",
    "# Create Row objects representing individual student records with 'id' and 'Name' fields\n",
    "r1 = Row(1, 'Sanket')   # Row with id=1 and Name='Sanket'\n",
    "r2 = Row(2, 'Akash')    # Row with id=2 and Name='Akash'\n",
    "r3 = Row(3, 'Rajesh')   # Row with id=3 and Name='Rajesh'\n",
    "r4 = Row(4, 'divya')    # Row with id=4 and Name='divya'\n",
    "\n",
    "# Create a DataFrame from the list of Row objects using the defined schema 'student'\n",
    "# The 'schema=student' argument assigns the column names 'id' and 'Name' to the DataFrame\n",
    "df1 = spark.createDataFrame([r1, r2, r3, r4], schema=student)  \n",
    "\n",
    "# Display the contents of the DataFrame in a tabular format\n",
    "df1.show()  \n",
    "\n",
    "# Print the schema of the DataFrame to show column names and data types\n",
    "df1.printSchema()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbed21dd-8928-4db9-8136-915153594e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n| name|          game|\n+-----+--------------+\n|nisha|    {chess, 2}|\n| nina|    {chess, 3}|\n| nila|{football, 10}|\n+-----+--------------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Importing the Row class from pyspark.sql to create Row objects\n",
    "# Row allows the creation of records similar to a dictionary or struct\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Creating a list named 'data' that contains Row objects\n",
    "# Each Row represents a record with two fields: 'name' and 'game'\n",
    "# The 'game' field itself is a nested Row containing 'g1' (game name) and 'rank' (player's rank)\n",
    "data = [\n",
    "    Row(name='nisha', game=Row(g1='chess', rank=2)),     # First record: name='nisha', plays 'chess' with rank=2\n",
    "    Row(name='nina', game=Row(g1='chess', rank=3)),      # Second record: name='nina', plays 'chess' with rank=3\n",
    "    Row(name='nila', game=Row(g1='football', rank=10))   # Third record: name='nila', plays 'football' with rank=10\n",
    "]\n",
    "\n",
    "# Creating a DataFrame named 'df' from the list of Row objects\n",
    "# The spark.createDataFrame() function takes the 'data' list and automatically infers the schema\n",
    "# The 'game' column will be interpreted as a struct type because it contains nested fields ('g1' and 'rank')\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Displaying the contents of the DataFrame in a tabular format\n",
    "# The show() function by default displays the first 20 rows\n",
    "# The 'game' column will show nested data as {g1_value, rank_value}\n",
    "df.show()\n",
    "\n",
    "# Printing the inferred schema of the DataFrame\n",
    "# The printSchema() function shows the structure of the DataFrame, including data types and nullability\n",
    "# The output will show that 'game' is a struct type with two fields: 'g1' (string) and 'rank' (long)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1796b6d8-18f4-48ff-b270-ba40c10fbf65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1️⃣ **Difference in Row Creation:**  \n",
    "- **First code:** Directly uses `Row()` with named parameters (`Row(name='nisha', game=Row(...))`).  \n",
    "- **Second code:** Uses a **Row factory** (`sg = Row('name', 'game')`) to create rows with a predefined structure.  \n",
    "\n",
    "2️⃣ **Schema Definition:**  \n",
    "- **First code:** Schema inferred from direct `Row` definitions.  \n",
    "- **Second code:** Schema is implicitly defined by the **Row factory (`sg`)**, ensuring a uniform structure across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f3ed814-df73-4c9a-a593-09c3e41f67c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n| name|      game|\n+-----+----------+\n|Nisha|{Chess, 2}|\n|Nisha|{Chess, 3}|\n|Nisha|{chess, 2}|\n+-----+----------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Importing the Row class from pyspark.sql to create Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Creating a Row factory 'sg' with two fields: 'name' and 'game'\n",
    "# This allows creating Row objects using 'sg' with the specified field names\n",
    "sg = Row('name', 'game')\n",
    "\n",
    "# Creating the first Row 'r1' using the Row factory 'sg'\n",
    "# 'name' field is assigned 'Nisha'\n",
    "# 'game' field is a nested Row containing 'g1' (game name = 'Chess') and 'rank' (rank = 2)\n",
    "r1 = sg('Nisha', Row(g1='Chess', rank=2))\n",
    "\n",
    "# Creating the second Row 'r2' with 'name' as 'Nisha' and 'game' as nested Row with rank 3\n",
    "r2 = sg('Nisha', Row(g1='Chess', rank=3))\n",
    "\n",
    "# Creating the third Row 'r3' with the same values as 'r1' for demonstration\n",
    "r3 = sg('Nisha', Row(g1='chess', rank=2))\n",
    "\n",
    "# Creating a DataFrame 'df2' from the list of Row objects [r1, r2, r3]\n",
    "# PySpark will infer the schema automatically, including the nested structure for 'game'\n",
    "df2 = spark.createDataFrame([r1, r2, r3])\n",
    "df2.show()\n",
    "# Printing the schema of the DataFrame to display the structure and data types of all columns\n",
    "# The 'game' column will appear as a struct with two fields: 'g1' (string) and 'rank' (long)\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301a27eb-8d58-4a38-8600-c3000db77a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Sanket\nRow(g1='sketing', rank='3')\n+---+------+------------+\n| id|  name|        rank|\n+---+------+------------+\n|  1|Sanket|{sketing, 3}|\n|  2| divya|{sketing, 2}|\n|  3|  wade|{sketing, 1}|\n+---+------+------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- rank: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Importing the Row class from pyspark.sql to create Row objects\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Creating a Row factory 'rank' with two fields: 'g1' (game) and 'rank' (player rank)\n",
    "# This allows creating nested Row objects for the 'rank' column with the specified structure\n",
    "rank = Row('g1', 'rank')\n",
    "\n",
    "# Creating another Row factory 'student' with three fields: 'id', 'name', and 'rank'\n",
    "# The 'rank' field here will store nested Row objects created using the 'rank' factory above\n",
    "student = Row('id', 'name', 'rank')\n",
    "\n",
    "# Creating the first Row 'r1' using the 'student' Row factory\n",
    "# 'id' = 1, 'name' = 'Sanket', 'rank' = nested Row with 'g1' = 'sketing' and 'rank' = '3'\n",
    "r1 = student(1, 'Sanket', rank('sketing', '3'))\n",
    "\n",
    "# Creating the second Row 'r2' with 'id' = 2, 'name' = 'divya', and 'rank' = ('sketing', '2')\n",
    "r2 = student(2, 'divya', rank('sketing', '2'))\n",
    "\n",
    "# Creating the third Row 'r3' with 'id' = 3, 'name' = 'wade', and 'rank' = ('sketing', '1')\n",
    "r3 = student(3, 'wade', rank('sketing', '1'))\n",
    "\n",
    "# Printing the 'id' and 'name' fields of the first Row (r1)\n",
    "# Output will be: 1 Sanket\n",
    "print(r1.id, r1.name)\n",
    "\n",
    "# Printing the entire 'rank' nested Row for r1\n",
    "# Output will show: Row(g1='sketing', rank='3')\n",
    "print(r1.rank)\n",
    "\n",
    "# Creating a list 'data1' containing all three Row objects [r1, r2, r3]\n",
    "data1 = [r1, r2, r3]\n",
    "\n",
    "# Creating a DataFrame 'df' from the list of Row objects\n",
    "# PySpark automatically infers the schema, including the nested struct for 'rank'\n",
    "df = spark.createDataFrame(data1)\n",
    "\n",
    "# Displaying the contents of the DataFrame in tabular form\n",
    "df.show()\n",
    "\n",
    "# Printing the inferred schema of the DataFrame\n",
    "# The 'rank' field will be shown as a struct with fields 'g1' (string) and 'rank' (string)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1bdcf9b-1718-4abb-bb55-9be0f01266ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ **Differences between the three codes in 3 lines:**\n",
    "\n",
    "1️⃣ **Row Creation Method:**  \n",
    "- **Code 1:** Directly uses **`Row()` with nested fields** for each record.  \n",
    "- **Code 2:** Uses a **Row factory (`sg`)** for a consistent schema across rows.  \n",
    "- **Code 3:** Defines **multiple Row factories (`rank`, `student`)** to create a **nested struct** with an additional `id` field.\n",
    "\n",
    "---\n",
    "\n",
    "2️⃣ **Data Structure & Schema:**  \n",
    "- **Code 1:** **Nested struct** (`game`) with varying values for `g1` and `rank`.  \n",
    "- **Code 2:** **Uniform nested struct** (`game`) across all rows with same `name`.  \n",
    "- **Code 3:** **Deeper nested struct** (`rank`) inside the `student` row, adding an **extra `id` field**.\n",
    "\n",
    "---\n",
    "\n",
    "3️⃣ **Data Variability:**  \n",
    "- **Code 1:** **Different names** and **different games** (`chess`, `football`).  \n",
    "- **Code 2:** **Same name** (`Nisha`), **same game** (`Chess`), but **different ranks**.  \n",
    "- **Code 3:** **Different IDs**, **same game** (`sketing`), **different ranks** for each student."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8822fceb-bf58-4c6c-91a4-538753ef4581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# COL operation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ec4e5db-3448-4df6-86c3-b0da4f99c1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ **Explanation of Commonly Used PySpark Functions**  \n",
    "\n",
    "These functions are imported from `pyspark.sql.functions` and are widely used for **DataFrame transformations**. Let’s break down the **syntax**, **usage**, and **examples** for each:  \n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ **`lit()` – Literal Value Function**  \n",
    "#### 🔹 **Purpose:**  \n",
    "- Adds a **constant (literal) value** to a column for all rows in a DataFrame.  \n",
    "- Commonly used with **`withColumn()`**.\n",
    "\n",
    "#### 🔹 **Syntax:**  \n",
    "```python\n",
    "lit(value)\n",
    "```\n",
    "- `value`: The **constant value** (string, number, boolean, etc.) you want to add.\n",
    "\n",
    "#### 🔹 **Example:**\n",
    "```python\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df = df.withColumn(\"Country\", lit(\"India\"))  # Adds a new column 'Country' with 'India' for all rows\n",
    "df.show()\n",
    "```\n",
    "#### 🔹 **Output:**\n",
    "```\n",
    "+---+------+---------+\n",
    "| id|  name|  Country|\n",
    "+---+------+---------+\n",
    "|  1|Sanket|    India|\n",
    "|  2|  Wade|    India|\n",
    "+---+------+---------+\n",
    "```\n",
    "---\n",
    "\n",
    "### 2️⃣ **`col()` – Column Reference Function**  \n",
    "#### 🔹 **Purpose:**  \n",
    "- Refers to a **column** in a DataFrame by name.  \n",
    "- Mostly used in **expressions**, **filters**, and **select** statements.\n",
    "\n",
    "#### 🔹 **Syntax:**  \n",
    "```python\n",
    "col(\"column_name\")\n",
    "```\n",
    "- `\"column_name\"`: The **name of the column** you want to reference.\n",
    "\n",
    "#### 🔹 **Example:**\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.select(col(\"name\"), col(\"id\")).show()  # Selects only 'name' and 'id' columns from the DataFrame\n",
    "```\n",
    "#### 🔹 **Output:**\n",
    "```\n",
    "+------+---+\n",
    "|  name| id|\n",
    "+------+---+\n",
    "|Sanket|  1|\n",
    "|  Wade|  2|\n",
    "+------+---+\n",
    "```\n",
    "---\n",
    "\n",
    "### 3️⃣ **`when()` – Conditional Logic Function**  \n",
    "#### 🔹 **Purpose:**  \n",
    "- Similar to **IF-ELSE** logic in SQL.  \n",
    "- Returns a **new column** based on **conditional statements**.  \n",
    "- Often used with **`otherwise()`** for the else condition.\n",
    "\n",
    "#### 🔹 **Syntax:**  \n",
    "```python\n",
    "when(condition, value).otherwise(value)\n",
    "```\n",
    "- `condition`: A **boolean expression** (e.g., `col('rank') > 2`).  \n",
    "- `value`: The **value** to assign if the condition is **true** or **false**.\n",
    "\n",
    "#### 🔹 **Example:**\n",
    "```python\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\"Performance\", when(col(\"rank\") > 2, \"Average\").otherwise(\"Good\"))\n",
    "df.show()\n",
    "```\n",
    "#### 🔹 **Output:**\n",
    "```\n",
    "+---+------+----+-----------+\n",
    "| id|  name|rank|Performance|\n",
    "+---+------+----+-----------+\n",
    "|  1|Sanket|   3|    Average|\n",
    "|  2|  Wade|   2|        Good|\n",
    "+---+------+----+-----------+\n",
    "```\n",
    "---\n",
    "\n",
    "### ⚡ **Summary of Syntax & Usage:**  \n",
    "| Function | Syntax                   | Purpose                      | Example Usage                      |\n",
    "|----------|--------------------------|------------------------------|-------------------------------------|\n",
    "| **`lit()`**  | `lit(value)`               | Adds **constant value** to a column | `df.withColumn('Country', lit('India'))` |\n",
    "| **`col()`**  | `col('column_name')`       | **References a column** by name     | `df.select(col('name'))`            |\n",
    "| **`when()`** | `when(condition, value)`   | Adds **conditional logic** to a column | `df.withColumn('Status', when(col('score') > 50, 'Pass').otherwise('Fail'))` |\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Real-World Example Combining All Functions:**\n",
    "```python\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(1, \"Sanket\", 3), (2, \"Wade\", 2)]\n",
    "columns = [\"id\", \"name\", \"rank\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Applying lit(), col(), and when() together\n",
    "df = df.withColumn(\"Country\", lit(\"India\")) \\\n",
    "       .withColumn(\"Performance\", when(col(\"rank\") > 2, \"Average\").otherwise(\"Good\"))\n",
    "\n",
    "df.show()\n",
    "```\n",
    "\n",
    "#### 🔹 **Output:**\n",
    "```\n",
    "+---+------+----+-------+-----------+\n",
    "| id|  name|rank|Country|Performance|\n",
    "+---+------+----+-------+-----------+\n",
    "|  1|Sanket|   3|  India|    Average|\n",
    "|  2|  Wade|   2|  India|        Good|\n",
    "+---+------+----+-------+-----------+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 **Key Takeaways:**\n",
    "- **`lit()`** is best for **constant values** across all rows.  \n",
    "- **`col()`** allows for **dynamic column referencing**, crucial for transformations.  \n",
    "- **`when()`** enables **conditional transformations**, adding **decision-making logic** to columns.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695ce70d-dcef-432e-8a17-efde8a745adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+--------+\n| id|  name|        rank|  School|\n+---+------+------------+--------+\n|  1|Sanket|{sketing, 3}|Symbosis|\n|  2| divya|{sketing, 2}|Symbosis|\n|  3|  wade|{sketing, 1}|Symbosis|\n+---+------+------------+--------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- rank: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: string (nullable = true)\n |-- School: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "# Importing all functions from the pyspark.sql.functions module\n",
    "# This includes functions like lit(), col(), when(), etc., used for DataFrame transformations\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adding a new column 'School' to the DataFrame 'df' using withColumn()\n",
    "# The 'lit()' function is used to assign a constant value ('Symbosis') to the new column for all rows\n",
    "df1 = df.withColumn('School', lit('Symbosis'))\n",
    "\n",
    "# Displaying the updated DataFrame with the newly added 'School' column\n",
    "df1.show()\n",
    "\n",
    "# Printing the updated schema of the DataFrame to confirm that 'School' has been added as a string field\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27324f36-4c6d-42ca-aab2-6c3946666da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n| ID|   Name|Score|\n+---+-------+-----+\n|  1|  Alice|   85|\n|  2|    Bob|   45|\n|  3|Charlie|   60|\n|  4|  David|   30|\n+---+-------+-----+\n\n+---+-------+---------------------------------------------+\n| ID|   Name|CASE WHEN (Score = 85) THEN 90 ELSE Score END|\n+---+-------+---------------------------------------------+\n|  1|  Alice|                                           90|\n|  2|    Bob|                                           45|\n|  3|Charlie|                                           60|\n|  4|  David|                                           30|\n+---+-------+---------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Sample data\n",
    "data = [     \n",
    "    (1, \"Alice\", 85),     \n",
    "    (2, \"Bob\", 45),     \n",
    "    (3, \"Charlie\", 60),     \n",
    "    (4, \"David\", 30) \n",
    "] \n",
    "\n",
    "# Define schema and create DataFrame \n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Score\"])\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show()\n",
    "\n",
    "# Apply conditional transformations on 'Score' column\n",
    "df1 = df.select(df.ID, df.Name, \n",
    "                when(df.Score == 85, 90).otherwise(df.Score))  # Correct the syntax for the 'when' function\n",
    "\n",
    "df1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c281d8b4-e0b4-4a5f-ae72-74d8602a1f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " ### `when(df.Score == 85, 90).otherwise(df.Score)`:\n",
    "\n",
    "- **`when()`**: This function is used to define conditional logic. It works like an **IF-ELSE** statement in programming.\n",
    "  - **`df.Score == 85`**: This checks if the value in the **Score** column is **85**.\n",
    "  - **`90`**: If the condition is **True** (i.e., if the score is **85**), the value will be replaced with **90**.\n",
    "\n",
    "- **`otherwise(df.Score)`**: This specifies what happens when the condition in `when()` is **False**.\n",
    "  - If the score is **not** 85, then it keeps the original value of **Score** from the DataFrame (i.e., `df.Score`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0096d7e6-950d-417e-9e02-a66fcf3a27e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n| ID|   Name|Score|\n+---+-------+-----+\n|  1|  Alice|   85|\n|  2|    Bob|   45|\n|  3|Charlie|   60|\n|  4|  David|   30|\n+---+-------+-----+\n\n+---+-------+-----------------------------------------------------------------------+\n| ID|   Name|CASE WHEN (Score = 85) THEN 90 WHEN (Score = 45) THEN 50 ELSE Score END|\n+---+-------+-----------------------------------------------------------------------+\n|  1|  Alice|                                                                     90|\n|  2|    Bob|                                                                     50|\n|  3|Charlie|                                                                     60|\n|  4|  David|                                                                     30|\n+---+-------+-----------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "data = [     (1, \"Alice\", 85),     (2, \"Bob\", 45),     (3, \"Charlie\", 60),     (4, \"David\", 30) ] \n",
    "# Define schema and create DataFrame \n",
    "df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Score\"])\n",
    "\n",
    "df.show()\n",
    "\n",
    "df1 = df.select(df.ID, df.Name, \n",
    "                when(df.Score == 85, 90)\n",
    "                .when(df.Score == 45, 50)  # second condition for when the score is 45\n",
    "                .otherwise(df.Score))  # keep the original score for all other cases\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b55aca7-a311-4465-b195-3e865ab341fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here's the short version without the output:\n",
    "\n",
    "```markdown\n",
    "### Sorting the DataFrame\n",
    "\n",
    "1. **Original DataFrame (Unsorted)**:\n",
    "```python\n",
    "df.show()\n",
    "```\n",
    "\n",
    "2. **Sort by 'id' in Ascending Order (Default)**:\n",
    "```python\n",
    "df.sort(df.id).show()  # Sorts from lowest to highest 'id'.(1,2,3)\n",
    "```\n",
    "\n",
    "3. **Sort by 'id' in Descending Order**:\n",
    "```python\n",
    "from pyspark.sql.functions import desc\n",
    "df.sort(desc(\"id\")).show()  # Sorts from highest to lowest 'id'.(3,2,1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43b9682-03da-483f-94bc-409a58e3c02e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1| vicky| 20000|\n|  2| divya| 50000|\n|  3|Sanket| 60000|\n+---+------+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "#### SORTING THE DATA\n",
    "\n",
    "# Define a list of tuples containing data (ID, Name, Salary)\n",
    "data1 = [(3, \"Sanket\", \"60000\"), (2, \"divya\", \"50000\"), (1, \"vicky\", \"20000\")]\n",
    "\n",
    "# Define the column names for the DataFrame\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "# Create a DataFrame `df` from the list of tuples `data1` using the specified column names `columns`\n",
    "df = spark.createDataFrame(data=data1, schema=columns)\n",
    "\n",
    "# Show the original DataFrame (unsorted)\n",
    "df.show() \n",
    "# Sort the DataFrame based on the 'id' column in ascending order and show the result\n",
    "df.sort(df.id).show() #when you use the sort(df.id) method, the default behavior is ascending (lowest to highest).\n",
    "\n",
    "# Sort the DataFrame based on the 'id' column in descending order and show the result\n",
    "df.sort(desc(\"id\")).show() #( dec:- from highest to lowest).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820f38a7-c91f-4a70-9635-dbc58954d5fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nNone\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n\nNone\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Select 'id', 'name', and 'salary' columns from the original DataFrame and cast 'salary' to an integer type\n",
    "df1 = df.select(df.id, df.name, df.salary.cast('int'))\n",
    "\n",
    "# Print the schema of df1 to check the data types of its columns\n",
    "print(df1.printSchema())\n",
    "\n",
    "# Print the schema of df to check the data types of its columns before casting\n",
    "print(df.printSchema())\n",
    "\n",
    "# Show the data of df1 after the type casting operation\n",
    "df1.show()\n",
    "\n",
    "# Show the data of the original DataFrame df (before type casting)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0339cb-bbe8-4464-a2c2-7b9f729efa6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Code Explanation:\n",
    "\n",
    "1. **`df.select(df.id, df.name, df.salary.cast('int'))`**  \n",
    "   - Selects the `id` and `name` columns, and casts the `salary` column to an **integer** type using `.cast('int')`.  \n",
    "   - This operation does not modify the original `df` DataFrame but creates a new DataFrame (`df1`) where `salary` is of type integer.\n",
    "\n",
    "2. **`df1.printSchema()`**  \n",
    "   - Prints the schema of the DataFrame `df1`, showing the column names and their **data types** after the type cast operation.  \n",
    "   - You will see that the `salary` column is now of type `int`.\n",
    "\n",
    "3. **`df.printSchema()`**  \n",
    "   - Prints the schema of the original DataFrame `df`, showing the column names and their **data types** before the type cast operation.  \n",
    "   - The `salary` column will remain in its original data type (e.g., `string` or `float`).\n",
    "\n",
    "4. **`df1.show()`**  \n",
    "   - Displays the data in the `df1` DataFrame, where the `salary` column has been **cast to integers**.  \n",
    "   - The other columns (`id` and `name`) remain unchanged.\n",
    "\n",
    "5. **`df.show()`**  \n",
    "   - Displays the data in the original `df` DataFrame, where the `salary` column remains in its **original format** (e.g., string or float, as it was before the cast).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c5ad26-703f-4698-b9a5-8ce6325f2bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Filter and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c9cb53-c40f-48be-a0ed-22cf63757846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame `df` to select only rows where the `name` column contains the letter 'a'.\n",
    "# The `like('%a%')` function is used to perform a case-sensitive pattern match.\n",
    "# The `%` symbols are wildcards that allow for any characters before or after the 'a'.\n",
    "df.filter(df.name.like('%a%')).show()  # Display the filtered rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa177b3-c0e2-4f23-ae52-dfa25e3a0e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  2|divya| 50000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.where((df.salary == 50000) & ~df.name.like('%t%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351a38fa-76d0-43bc-9ce4-569c4ede335f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  2|divya| 50000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is exactly 50000 and the `name` does not contain 't'.\n",
    "df.where((df.salary == 50000) & ~(df.name.like('%t%'))).show()\n",
    "#correct ouput divya in sanket contain  \"t\" and in vicky| 20000 sal is low they rejected\n",
    "# and (&) required both condiation true\n",
    "#  ~ (not operator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e707f7-1c4a-4ef2-a76f-fba8edd827f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### `like('%t%')` Explanation\n",
    "\n",
    "The `like('%t%')` expression is used to filter rows in PySpark where the string in a particular column contains the character `t`.\n",
    "\n",
    "#### Breakdown:\n",
    "- **`%` (Percent Sign)**: The percent sign is a wildcard in SQL-like queries that matches zero or more characters.\n",
    "- **`t`**: This is the character you want to check within the column values.\n",
    "\n",
    "#### Example:\n",
    "- `df.name.like('%t%')` will match any row where the `name` column contains the letter 't' at any position in the string. For example, it would match 'Sanket', 'Vicky', 'Martha', etc., because 't' is present in those names.\n",
    "  \n",
    "#### Negation:\n",
    "If you use negation (`~`), like `~(df.name.like('%t%'))`, it will **exclude** rows where the `name` contains 't'.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "# Include rows where the 'name' column contains 't'\n",
    "df.where(df.name.like('%t%')).show()\n",
    "\n",
    "# Exclude rows where the 'name' column contains 't'\n",
    "df.where(~(df.name.like('%t%'))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d5f812f-81cc-482a-8a23-04a8524a0817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n| id|name|salary|\n+---+----+------+\n+---+----+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Filter rows where salary is greater than or equal to 60000 and the `name` does not contain 'sanket'.\n",
    "df.where((df.salary >= 60000) & ~(df.name.like('%Sanket%'))).show()\n",
    "# and (&) required both condiation true\n",
    "#  ~ (not operator)\n",
    "df.where((df.salary >= 60000) & ~(df.name.like('%sanket%'))).show() # s is lower case in orignal data s is capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f3f26c3-9adc-4e71-aaf6-e0057a4cca14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|vicky| 20000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is less than 50000 and the `name` does not contain 'san'.\n",
    "df.where((df.salary < 50000) & ~(df.name.like('%san%'))).show() \n",
    "#The expression like('%san%') filters rows where the column contains the substring 'san' at any position in the string.\n",
    "# and (&) required both condiation true\n",
    "#  ~ (not operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095af9bb-86d4-4e79-9ea6-c60f95d6a1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is greater than or equal to 50000 or the `name` does not contain 't'.\n",
    "df.where((df.salary >= 50000) | ~(df.name.like('%t%'))).show()\n",
    "# or (|) required only one condiation true\n",
    "#  ~ (not operator)\n",
    "#The expression `like('%t%')` filters rows where the column contains the letter 't' at any position in the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3b65f87-6fd0-4b6d-b8be-bb97f94bbd3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sure! Here's the explanation in Markdown format:\n",
    "\n",
    "### Why \"Sanket\" is included:\n",
    "\n",
    "For **\"Sanket\"**:\n",
    "- **`df.salary >= 50000`**: The salary is 60000, so this condition **evaluates to true**.\n",
    "- **`~(df.name.like('%t%'))`**: The name contains \"t\", so this condition **evaluates to false**.\n",
    "\n",
    "Since the **OR (`|`)** operator requires **only one condition to be true**, the row is included in the result because the first condition (**salary >= 50000**) is true.\n",
    "\n",
    "Let me know if you need further clarification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e74b7b-240a-48b7-ad3c-925d4602fe75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is greater than or equal to 50000 \n",
    "# or where salary is less than 50000 and the name does not contain 't'.\n",
    "df.where((df.salary >= 50000) | ((df.salary < 50000) & ~(df.name.like('%t%')))).show()\n",
    "#The expression `like('%t%')` filters rows where the column contains the letter 't' at any position in the string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff2ee59-75a6-4166-b25f-c89c8ef4b428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Condition 1: `df.salary >= 50000`\n",
    "\n",
    "- **Sanket** has a salary of 60000, which satisfies this condition because 60000 is greater than or equal to 50000.\n",
    "- Since **Sanket** satisfies this condition, the row will be included regardless of the second condition.\n",
    "\n",
    "### Condition 2: `((df.salary < 50000) & ~(df.name.like('%t%')))`\n",
    "\n",
    "- This condition applies only when the salary is less than 50000. It also checks if the name **does not** contain the letter 't'.\n",
    "- Since **Sanket**'s salary is 60000, this condition is **not evaluated** for him (because of the OR condition, as it is already included by the first condition).\n",
    "\n",
    "### Key Point:\n",
    "- **OR Condition**: The **OR (`|`)** operator means that if **either of the two conditions** is **true**, the row is included. Since **Sanket**'s salary is >= 50000, the row is included regardless of the second condition.\n",
    "\n",
    "### Conclusion:\n",
    "- **Sanket** is included because he meets the first condition (`df.salary >= 50000`), and the second condition is not needed in this case due to the OR operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a142d8-7765-48ac-8c42-000c918ae93b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is greater than or equal to 50000 or the `name` does not contain 'divya'.\n",
    "df.where((df.salary >= 50000) | ~(df.name.like('%divya%'))).show()\n",
    "#here or operator use so it check 1st condition is true then not check 2nd condition so in ouput we get divya\n",
    "# in case of vickey 1st condition false so check 2nd is true so vickey in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3320eb56-4abe-4ff4-8e7a-f8511a976eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  2|divya| 50000|\n+---+-----+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n+---+------+------+\n\n+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|vicky| 20000|\n+---+-----+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where salary is exactly 50000 and the `name` does not contain 't'.\n",
    "df.where((df.salary == 50000) & ~(df.name.like('%t%'))).show() \n",
    "\n",
    "# Filter rows where salary is greater than or equal to 60000 and the `name` does not contain 'sanket'.\n",
    "df.where((df.salary >= 60000) & ~(df.name.like('%sanket%'))).show()\n",
    "\n",
    "# Filter rows where salary is less than 50000 and the `name` does not contain 'san'.\n",
    "df.where((df.salary < 50000) & ~(df.name.like('%san%'))).show()\n",
    "\n",
    "# Filter rows where salary is greater than or equal to 50000 or the `name` does not contain 't'.\n",
    "df.where((df.salary >= 50000) | ~(df.name.like('%t%'))).show()\n",
    "\n",
    "# Filter rows where salary is greater than or equal to 50000 or the `name` does not contain 'divya'.\n",
    "df.where((df.salary >= 50000) | ~(df.name.like('%divya%'))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8246f749-beec-4fdd-aa7b-40aa716464e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.where( (df.salary >50000) & ~(df.name.like('%v%'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba3942d-2057-4bf3-af31-8374d434044c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  3|Sanket| 60000|\n|  2| divya| 50000|\n|  1| vicky| 20000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['name']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efcab7c7-5b40-4927-bb33-f63ad2da4c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+---+-------------+----------+-----+------+---+-----+\n|id |employee_name|department|state|salary|age|bonus|\n+---+-------------+----------+-----+------+---+-----+\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Michael      |Sales     |NY   |86000 |56 |20000|\n|3  |Robert       |Sales     |CA   |81000 |30 |23000|\n|4  |Maria        |Finance   |CA   |90000 |24 |23000|\n+---+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing the necessary PySpark libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a SparkSession, which is the entry point for using Spark SQL\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Sample data as a list of tuples with employee information\n",
    "simpleData = [(1,\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (2,\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (3,\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (4,\"Maria\",\"Finance\",\"CA\",90000,24,23000) \\\n",
    "  ]\n",
    "\n",
    "# Defining the column names for the DataFrame\n",
    "columns = [\"id\",\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "\n",
    "# Creating a DataFrame using the sample data and column names\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "\n",
    "# Printing the schema of the DataFrame to show the column types\n",
    "df.printSchema()\n",
    "\n",
    "# Displaying the DataFrame in a tabular format with no truncation\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05149e6c-b917-4977-8117-66a5009177f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Explanation:\n",
    "\n",
    "#### **Import Libraries:**\n",
    "\n",
    "- **`import pyspark`**: Import the `pyspark` module to interact with Spark.\n",
    "- **`from pyspark.sql import SparkSession`**: Import `SparkSession`, which provides the entry point for using Spark SQL.\n",
    "\n",
    "#### **Creating a SparkSession:**\n",
    "\n",
    "- **`spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()`**: This creates a `SparkSession` with the application name `'SparkByExamples.com'`. The `getOrCreate()` method either retrieves an existing `SparkSession` or creates a new one.\n",
    "\n",
    "#### **Sample Data:**\n",
    "\n",
    "- **`simpleData`**: A list of tuples containing employee data. Each tuple represents a row in the DataFrame.\n",
    "  - The columns in the data are:\n",
    "    - **`id`**: Employee ID\n",
    "    - **`employee_name`**: Name of the employee\n",
    "    - **`department`**: Department the employee works in\n",
    "    - **`state`**: Location of the employee's office\n",
    "    - **`salary`**: Employee's salary\n",
    "    - **`age`**: Age of the employee\n",
    "    - **`bonus`**: Bonus of the employee\n",
    "\n",
    "#### **Defining Column Names:**\n",
    "\n",
    "- **`columns`**: A list that defines the column names corresponding to the data.\n",
    "\n",
    "#### **Creating a DataFrame:**\n",
    "\n",
    "- **`df = spark.createDataFrame(data = simpleData, schema = columns)`**: This creates a PySpark DataFrame from the provided data (`simpleData`) and schema (`columns`).\n",
    "\n",
    "#### **Printing Schema:**\n",
    "\n",
    "- **`df.printSchema()`**: This method prints the schema of the DataFrame, showing the column names and their data types.\n",
    "\n",
    "#### **Displaying Data:**\n",
    "\n",
    "- **`df.show(truncate=True)`**: This displays the contents of the DataFrame in a tabular format, but with truncation enabled. If any column contains long text or values, it will show only the first 20 characters. This is useful to prevent data from being cut off in case of long strings.\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **`show(truncate=True)`**: This limits the display of each column value to 20 characters by default. If you want to see the entire content of each column, you can set `truncate=False`.\n",
    "  \n",
    "- **`printSchema()`**: Helps in understanding the structure of the DataFrame by showing column names and their respective data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c64c64e-04dd-4772-908d-4e31306f69a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- employee_name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- state: string (nullable = true)\n |-- salary: long (nullable = true)\n |-- age: long (nullable = true)\n |-- bonus: long (nullable = true)\n\n+---+-------------+----------+-----+------+---+-----+\n|id |employee_name|department|state|salary|age|bonus|\n+---+-------------+----------+-----+------+---+-----+\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Maria        |Finance   |CA   |90000 |24 |23000|\n|3  |Jen          |Finance   |NY   |79000 |53 |15000|\n|4  |Jeff         |Marketing |CA   |80000 |25 |18000|\n|5  |Kumar        |Marketing |NY   |91000 |50 |21000|\n+---+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame2\n",
    "# Define a list of tuples containing employee data for the second DataFrame\n",
    "simpleData2 = [(1,\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (2,\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (3,\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (4,\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (5,\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "\n",
    "# Define column names corresponding to the data\n",
    "columns2 = [\"id\", \"employee_name\", \"department\", \"state\", \"salary\", \"age\", \"bonus\"]\n",
    "\n",
    "# Create the second DataFrame (df2) using the provided data and schema\n",
    "df2 = spark.createDataFrame(data = simpleData2, schema = columns2)\n",
    "\n",
    "# Print the schema of the DataFrame (shows the column names and their data types)\n",
    "df2.printSchema()\n",
    "\n",
    "# Show the contents of the DataFrame without truncating column values\n",
    "df2.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c8a3d7-f8c5-461b-83f6-3a7a25143310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+-----+------+---+-----+\n|id |employee_name|department|state|salary|age|bonus|\n+---+-------------+----------+-----+------+---+-----+\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Michael      |Sales     |NY   |86000 |56 |20000|\n|3  |Robert       |Sales     |CA   |81000 |30 |23000|\n|4  |Maria        |Finance   |CA   |90000 |24 |23000|\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Maria        |Finance   |CA   |90000 |24 |23000|\n|3  |Jen          |Finance   |NY   |79000 |53 |15000|\n|4  |Jeff         |Marketing |CA   |80000 |25 |18000|\n|5  |Kumar        |Marketing |NY   |91000 |50 |21000|\n+---+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# union() to merge two DataFrames\n",
    "unionDF = df.union(df2)\n",
    "unionDF.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7709ed40-d8c1-4551-9e36-d94c854717d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------+-----+------+---+-----+\n|id |employee_name|department|state|salary|age|bonus|\n+---+-------------+----------+-----+------+---+-----+\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Michael      |Sales     |NY   |86000 |56 |20000|\n|3  |Robert       |Sales     |CA   |81000 |30 |23000|\n|4  |Maria        |Finance   |CA   |90000 |24 |23000|\n|1  |James        |Sales     |NY   |90000 |34 |10000|\n|2  |Maria        |Finance   |CA   |90000 |24 |23000|\n|3  |Jen          |Finance   |NY   |79000 |53 |15000|\n|4  |Jeff         |Marketing |CA   |80000 |25 |18000|\n|5  |Kumar        |Marketing |NY   |91000 |50 |21000|\n+---+-------------+----------+-----+------+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# unionAll() to merge two DataFrames\n",
    "unionAllDF = df.unionAll(df2)\n",
    "unionAllDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26b1fb0-bb82-48d7-8f1d-87747723d585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Difference Between `UNION` and `UNION ALL` in PySpark**\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 **1. Definition**  \n",
    "- **`UNION`**:  \n",
    "  Combines the result sets of two DataFrames and removes any duplicate rows, returning only distinct records.  \n",
    "- **`UNION ALL`**:  \n",
    "  Combines the result sets of two DataFrames **without** removing duplicates, meaning all records (including duplicates) are returned.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **2. Syntax in PySpark**  \n",
    "```python\n",
    "# UNION - Removes duplicates\n",
    "df_union = df1.union(df2).distinct()\n",
    "\n",
    "# UNION ALL - Keeps duplicates\n",
    "df_union_all = df1.union(df2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ **3. Key Differences**\n",
    "\n",
    "| Feature            | `UNION`                         | `UNION ALL`                      |\n",
    "|--------------------|---------------------------------|-----------------------------------|\n",
    "| **Duplicates**     | Removes duplicates              | Includes duplicates               |\n",
    "| **Performance**    | Slower (due to duplicate check) | Faster (no duplicate check)       |\n",
    "| **Usage Scenario** | When unique records are needed  | When all records (including duplicates) are needed |\n",
    "| **Memory Usage**   | Higher (due to extra processing)| Lower (no extra processing)       |\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **4. Example**\n",
    "\n",
    "#### 📋 **DataFrames Example**  \n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UnionExample\").getOrCreate()\n",
    "\n",
    "data1 = [(1, \"James\"), (2, \"Maria\")]\n",
    "data2 = [(2, \"Maria\"), (3, \"Jen\")]\n",
    "\n",
    "columns = [\"id\", \"name\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, columns)\n",
    "df2 = spark.createDataFrame(data2, columns)\n",
    "\n",
    "# UNION (removes duplicates)\n",
    "df_union = df1.union(df2).distinct()\n",
    "df_union.show()\n",
    "\n",
    "# UNION ALL (includes duplicates)\n",
    "df_union_all = df1.union(df2)\n",
    "df_union_all.show()\n",
    "```\n",
    "\n",
    "#### 📊 **Output**\n",
    "\n",
    "- **UNION** (Distinct results):\n",
    "```\n",
    "+---+-----+\n",
    "| id| name|\n",
    "+---+-----+\n",
    "|  1|James|\n",
    "|  2|Maria|\n",
    "|  3|  Jen|\n",
    "+---+-----+\n",
    "```\n",
    "\n",
    "- **UNION ALL** (All results including duplicates):\n",
    "```\n",
    "+---+-----+\n",
    "| id| name|\n",
    "+---+-----+\n",
    "|  1|James|\n",
    "|  2|Maria|\n",
    "|  2|Maria|\n",
    "|  3|  Jen|\n",
    "+---+-----+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 **5. Performance Considerations**  \n",
    "- Use **`UNION ALL`** when duplicates are acceptable to gain better performance.  \n",
    "- Use **`UNION`** when you need a unique set of records but be aware it may affect performance due to the distinct operation.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 **6. Summary**  \n",
    "- ✅ **`UNION`** ensures unique results but can be slower.  \n",
    "- 🚀 **`UNION ALL`** performs faster but includes duplicates.  \n",
    "- 🎯 Choose based on the **data integrity** requirements and **performance** needs of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85b5ec2a-2284-4c98-adb5-c91b83eb6c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td></tr><tr><td>2</td><td>Michael</td><td>Sales</td><td>NY</td><td>86000</td><td>56</td><td>20000</td><td>2</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td></tr><tr><td>3</td><td>Robert</td><td>Sales</td><td>CA</td><td>81000</td><td>30</td><td>23000</td><td>3</td><td>Jen</td><td>Finance</td><td>NY</td><td>79000</td><td>53</td><td>15000</td></tr><tr><td>4</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td><td>4</td><td>Jeff</td><td>Marketing</td><td>CA</td><td>80000</td><td>25</td><td>18000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000,
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000
        ],
        [
         2,
         "Michael",
         "Sales",
         "NY",
         86000,
         56,
         20000,
         2,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000
        ],
        [
         3,
         "Robert",
         "Sales",
         "CA",
         81000,
         30,
         23000,
         3,
         "Jen",
         "Finance",
         "NY",
         79000,
         53,
         15000
        ],
        [
         4,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000,
         4,
         "Jeff",
         "Marketing",
         "CA",
         80000,
         25,
         18000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "in_join = df.join(df2,df.id == df2.id, 'inner')\n",
    "display(in_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e7be25-8328-4d47-9fb5-ad9a4dce965c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td></tr><tr><td>2</td><td>Michael</td><td>Sales</td><td>NY</td><td>86000</td><td>56</td><td>20000</td><td>2</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td></tr><tr><td>3</td><td>Robert</td><td>Sales</td><td>CA</td><td>81000</td><td>30</td><td>23000</td><td>3</td><td>Jen</td><td>Finance</td><td>NY</td><td>79000</td><td>53</td><td>15000</td></tr><tr><td>4</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td><td>4</td><td>Jeff</td><td>Marketing</td><td>CA</td><td>80000</td><td>25</td><td>18000</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5</td><td>Kumar</td><td>Marketing</td><td>NY</td><td>91000</td><td>50</td><td>21000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000,
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000
        ],
        [
         2,
         "Michael",
         "Sales",
         "NY",
         86000,
         56,
         20000,
         2,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000
        ],
        [
         3,
         "Robert",
         "Sales",
         "CA",
         81000,
         30,
         23000,
         3,
         "Jen",
         "Finance",
         "NY",
         79000,
         53,
         15000
        ],
        [
         4,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000,
         4,
         "Jeff",
         "Marketing",
         "CA",
         80000,
         25,
         18000
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         5,
         "Kumar",
         "Marketing",
         "NY",
         91000,
         50,
         21000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "r_join = df.join(df2,df.id == df2.id, 'right')\n",
    "display(r_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3bd025f-b906-45a3-8460-7931473b337e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td></tr><tr><td>2</td><td>Michael</td><td>Sales</td><td>NY</td><td>86000</td><td>56</td><td>20000</td><td>2</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td></tr><tr><td>3</td><td>Robert</td><td>Sales</td><td>CA</td><td>81000</td><td>30</td><td>23000</td><td>3</td><td>Jen</td><td>Finance</td><td>NY</td><td>79000</td><td>53</td><td>15000</td></tr><tr><td>4</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td><td>4</td><td>Jeff</td><td>Marketing</td><td>CA</td><td>80000</td><td>25</td><td>18000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000,
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000
        ],
        [
         2,
         "Michael",
         "Sales",
         "NY",
         86000,
         56,
         20000,
         2,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000
        ],
        [
         3,
         "Robert",
         "Sales",
         "CA",
         81000,
         30,
         23000,
         3,
         "Jen",
         "Finance",
         "NY",
         79000,
         53,
         15000
        ],
        [
         4,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000,
         4,
         "Jeff",
         "Marketing",
         "CA",
         80000,
         25,
         18000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "l_join = df.join(df2,df.id == df2.id, 'left')\n",
    "display(l_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b769ff4-a911-422a-952c-7bd15a0e784c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th><th>id</th><th>employee_name</th><th>department</th><th>state</th><th>salary</th><th>age</th><th>bonus</th></tr></thead><tbody><tr><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td><td>1</td><td>James</td><td>Sales</td><td>NY</td><td>90000</td><td>34</td><td>10000</td></tr><tr><td>2</td><td>Michael</td><td>Sales</td><td>NY</td><td>86000</td><td>56</td><td>20000</td><td>2</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td></tr><tr><td>3</td><td>Robert</td><td>Sales</td><td>CA</td><td>81000</td><td>30</td><td>23000</td><td>3</td><td>Jen</td><td>Finance</td><td>NY</td><td>79000</td><td>53</td><td>15000</td></tr><tr><td>4</td><td>Maria</td><td>Finance</td><td>CA</td><td>90000</td><td>24</td><td>23000</td><td>4</td><td>Jeff</td><td>Marketing</td><td>CA</td><td>80000</td><td>25</td><td>18000</td></tr><tr><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>5</td><td>Kumar</td><td>Marketing</td><td>NY</td><td>91000</td><td>50</td><td>21000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000,
         1,
         "James",
         "Sales",
         "NY",
         90000,
         34,
         10000
        ],
        [
         2,
         "Michael",
         "Sales",
         "NY",
         86000,
         56,
         20000,
         2,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000
        ],
        [
         3,
         "Robert",
         "Sales",
         "CA",
         81000,
         30,
         23000,
         3,
         "Jen",
         "Finance",
         "NY",
         79000,
         53,
         15000
        ],
        [
         4,
         "Maria",
         "Finance",
         "CA",
         90000,
         24,
         23000,
         4,
         "Jeff",
         "Marketing",
         "CA",
         80000,
         25,
         18000
        ],
        [
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         5,
         "Kumar",
         "Marketing",
         "NY",
         91000,
         50,
         21000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "employee_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f_join = df.join(df2,df.id == df2.id, 'full')\n",
    "display(f_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cac6020-9a89-4ed5-931c-f94aa2c051ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PySpark Joins Explained (With Real-World Banking Examples)\n",
    "\n",
    "## 1. **Inner Join**\n",
    "### Description:\n",
    "Returns rows that have matching values in both DataFrames.\n",
    "\n",
    "### Real-World Example:\n",
    "*Finding customers with loan details available in both customer and loan DataFrames.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PySpark Joins\").getOrCreate()\n",
    "\n",
    "data_customers = [(1, \"James\"), (2, \"Maria\"), (3, \"Jen\")]\n",
    "data_loans = [(1, \"Personal Loan\"), (2, \"Home Loan\"), (4, \"Car Loan\")]\n",
    "\n",
    "columns_customers = [\"id\", \"name\"]\n",
    "columns_loans = [\"id\", \"loan_type\"]\n",
    "\n",
    "df_customers = spark.createDataFrame(data_customers, columns_customers)\n",
    "df_loans = spark.createDataFrame(data_loans, columns_loans)\n",
    "\n",
    "inner_join_df = df_customers.join(df_loans, on=\"id\", how=\"inner\")\n",
    "inner_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+-------------+\n",
    "| id| name|    loan_type|\n",
    "+---+-----+-------------+\n",
    "|  1|James|Personal Loan|\n",
    "|  2|Maria|   Home Loan |\n",
    "+---+-----+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Only customers who have loan records in the loan DataFrame are returned.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Left Outer Join**\n",
    "### Description:\n",
    "Returns all records from the left DataFrame, with matched records from the right DataFrame. Nulls for non-matches.\n",
    "\n",
    "### Real-World Example:\n",
    "*List all customers with their loan details, showing null where loans are not available.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "left_outer_join_df = df_customers.join(df_loans, on=\"id\", how=\"left\")\n",
    "left_outer_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+-------------+\n",
    "| id| name|    loan_type|\n",
    "+---+-----+-------------+\n",
    "|  1|James|Personal Loan|\n",
    "|  2|Maria|   Home Loan |\n",
    "|  3|  Jen|         null |\n",
    "+---+-----+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Shows all customers even if they don’t have loan records.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. **Right Outer Join**\n",
    "### Description:\n",
    "Returns all records from the right DataFrame, with matched records from the left DataFrame. Nulls for non-matches.\n",
    "\n",
    "### Real-World Example:\n",
    "*Display all loan types, including those with no associated customer.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "right_outer_join_df = df_customers.join(df_loans, on=\"id\", how=\"right\")\n",
    "right_outer_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+-------------+\n",
    "| id| name|    loan_type|\n",
    "+---+-----+-------------+\n",
    "|  1|James|Personal Loan|\n",
    "|  2|Maria|   Home Loan |\n",
    "|  4| null|     Car Loan|\n",
    "+---+-----+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Displays all loans, even those not assigned to any customer.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. **Full Outer Join**\n",
    "### Description:\n",
    "Returns all records when there’s a match in either DataFrame, filling nulls where no match exists.\n",
    "\n",
    "### Real-World Example:\n",
    "*Combine all customer and loan records to show full data availability.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "full_outer_join_df = df_customers.join(df_loans, on=\"id\", how=\"outer\")\n",
    "full_outer_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+-------------+\n",
    "| id| name|    loan_type|\n",
    "+---+-----+-------------+\n",
    "|  1|James|Personal Loan|\n",
    "|  2|Maria|   Home Loan |\n",
    "|  3|  Jen|         null |\n",
    "|  4| null|     Car Loan|\n",
    "+---+-----+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Combines the complete data from both DataFrames.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. **Left Semi Join**\n",
    "### Description:\n",
    "Returns only rows from the left DataFrame with a match in the right DataFrame.\n",
    "\n",
    "### Real-World Example:\n",
    "*Find customers who have at least one loan.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "left_semi_join_df = df_customers.join(df_loans, on=\"id\", how=\"left_semi\")\n",
    "left_semi_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+\n",
    "| id| name|\n",
    "+---+-----+\n",
    "|  1|James|\n",
    "|  2|Maria|\n",
    "+---+-----+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Only returns customers who have loan entries.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. **Left Anti Join**\n",
    "### Description:\n",
    "Returns only rows from the left DataFrame with no match in the right DataFrame.\n",
    "\n",
    "### Real-World Example:\n",
    "*Find customers who do not have any loans.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "left_anti_join_df = df_customers.join(df_loans, on=\"id\", how=\"left_anti\")\n",
    "left_anti_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+----+\n",
    "| id|name|\n",
    "+---+----+\n",
    "|  3| Jen|\n",
    "+---+----+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Shows customers who have no associated loans.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Cross Join**\n",
    "### Description:\n",
    "Returns the Cartesian product of both DataFrames (all combinations).\n",
    "\n",
    "### Real-World Example:\n",
    "*Generate all possible combinations of customers and loan types for offer planning.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "cross_join_df = df_customers.crossJoin(df_loans)\n",
    "cross_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+---+-------------+\n",
    "| id| name| id|    loan_type|\n",
    "+---+-----+---+-------------+\n",
    "|  1|James|  1|Personal Loan|\n",
    "|  1|James|  2|   Home Loan |\n",
    "|  1|James|  4|     Car Loan|\n",
    "|  2|Maria|  1|Personal Loan|\n",
    "|  2|Maria|  2|   Home Loan |\n",
    "|  2|Maria|  4|     Car Loan|\n",
    "|  3|  Jen|  1|Personal Loan|\n",
    "|  3|  Jen|  2|   Home Loan |\n",
    "|  3|  Jen|  4|     Car Loan|\n",
    "+---+-----+---+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- Generates all possible customer-loan pairings.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Broadcast Join**\n",
    "### Description:\n",
    "An optimized join where the smaller DataFrame is broadcast to all nodes, reducing shuffle operations.\n",
    "\n",
    "### Real-World Example:\n",
    "*Quickly join a large transaction DataFrame with a small loan type DataFrame for efficient querying.*\n",
    "\n",
    "### Code Example:\n",
    "```python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "broadcast_join_df = df_customers.join(broadcast(df_loans), on=\"id\", how=\"inner\")\n",
    "broadcast_join_df.show()\n",
    "```\n",
    "\n",
    "### Output:\n",
    "```\n",
    "+---+-----+-------------+\n",
    "| id| name|    loan_type|\n",
    "+---+-----+-------------+\n",
    "|  1|James|Personal Loan|\n",
    "|  2|Maria|   Home Loan |\n",
    "+---+-----+-------------+\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "- `broadcast()` ensures the smaller DataFrame is available on all nodes, improving join performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways:\n",
    "- **Inner Join:** Match records in both DataFrames.\n",
    "- **Left/Right Outer Joins:** Keep all records from one DataFrame with matched data from the other.\n",
    "- **Full Outer Join:** All records from both DataFrames with nulls for missing matches.\n",
    "- **Left Semi Join:** Only left DataFrame records with a match.\n",
    "- **Left Anti Join:** Left DataFrame records without a match.\n",
    "- **Cross Join:** All possible row combinations (Cartesian product).\n",
    "- **Broadcast Join:** Efficient for joining large DataFrames with small ones, reducing shuffle.\n",
    "\n",
    "These join strategies, paired with real-world banking examples, enable flexible and efficient data analysis using PySpark, ensuring optimized performance in distributed environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a876f1d-9660-4985-9281-c398d69b8c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 📖 **All Joins in PySpark - Detailed Explanation**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 **1. Inner Join**  \n",
    "- **Definition**: Returns records that have matching values in both DataFrames based on the given condition.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"inner\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+---+-----+\n",
    "  | id| name| id|dept |\n",
    "  +---+-----+---+-----+\n",
    "  |  1|John |  1|Sales|\n",
    "  |  2|Mike |  2|HR   |\n",
    "  +---+-----+---+-----+\n",
    "  ```\n",
    "- **Key Point**: Only matching rows from both DataFrames are returned.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 **2. Left Outer Join (Left Join)**  \n",
    "- **Definition**: Returns all rows from the **left** DataFrame and matching rows from the right DataFrame. Missing matches return `null`.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"left\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+-----+\n",
    "  | id| name|dept |\n",
    "  +---+-----+-----+\n",
    "  |  1|John |Sales|\n",
    "  |  2|Mike |HR   |\n",
    "  |  3|Sara |null |\n",
    "  +---+-----+-----+\n",
    "  ```\n",
    "- **Key Point**: All records from the left DataFrame are preserved.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌍 **3. Right Outer Join (Right Join)**  \n",
    "- **Definition**: Returns all rows from the **right** DataFrame and matching rows from the left DataFrame.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"right\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+-----+\n",
    "  | id| name|dept |\n",
    "  +---+-----+-----+\n",
    "  |  1|John |Sales|\n",
    "  |  2|Mike |HR   |\n",
    "  |  4|null |Admin|\n",
    "  +---+-----+-----+\n",
    "  ```\n",
    "- **Key Point**: All records from the right DataFrame are included.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 **4. Full Outer Join (Full Join)**  \n",
    "- **Definition**: Returns all rows from both DataFrames. If there is no match, returns `null` on the side without a match.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"outer\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+-----+\n",
    "  | id| name|dept |\n",
    "  +---+-----+-----+\n",
    "  |  1|John |Sales|\n",
    "  |  2|Mike |HR   |\n",
    "  |  3|Sara |null |\n",
    "  |  4|null |Admin|\n",
    "  +---+-----+-----+\n",
    "  ```\n",
    "- **Key Point**: Combines the effects of both left and right joins.\n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ **5. Left Semi Join**  \n",
    "- **Definition**: Returns rows from the **left** DataFrame where a match exists in the right DataFrame. Only left DataFrame columns are returned.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"left_semi\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+\n",
    "  | id| name|\n",
    "  +---+-----+\n",
    "  |  1|John |\n",
    "  |  2|Mike |\n",
    "  +---+-----+\n",
    "  ```\n",
    "- **Key Point**: Similar to an inner join but returns only columns from the left DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 **6. Left Anti Join**  \n",
    "- **Definition**: Returns only the rows from the **left** DataFrame that **do not** have a match in the right DataFrame.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, df1.id == df2.id, \"left_anti\")\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+\n",
    "  | id| name|\n",
    "  +---+-----+\n",
    "  |  3|Sara |\n",
    "  +---+-----+\n",
    "  ```\n",
    "- **Key Point**: Opposite of the left semi join.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **7. Cross Join (Cartesian Product)**  \n",
    "- **Definition**: Returns the Cartesian product of both DataFrames. Every row of the first DataFrame is joined with every row of the second DataFrame.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.crossJoin(df2).show()\n",
    "  ```  \n",
    "- **Example**:\n",
    "  ```python\n",
    "  +---+-----+---+-----+\n",
    "  | id| name| id|dept |\n",
    "  +---+-----+---+-----+\n",
    "  |  1|John |  1|Sales|\n",
    "  |  1|John |  2|HR   |\n",
    "  |  2|Mike |  1|Sales|\n",
    "  |  2|Mike |  2|HR   |\n",
    "  |  3|Sara |  1|Sales|\n",
    "  |  3|Sara |  2|HR   |\n",
    "  +---+-----+---+-----+\n",
    "  ```\n",
    "- **Key Point**: Be cautious—can produce a large number of rows.\n",
    "\n",
    "---\n",
    "\n",
    "### 📝 **8. Self Join**  \n",
    "- **Definition**: A join where a DataFrame is joined with itself. Useful for comparing rows within the same DataFrame.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df.alias(\"a\").join(df.alias(\"b\"), col(\"a.id\") == col(\"b.manager_id\"), \"inner\")\n",
    "  ```  \n",
    "- **Key Point**: Use aliases to differentiate the same DataFrame when joining.\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **9. Natural Join**  \n",
    "- **Definition**: Joins two DataFrames based on all columns with the same name.  \n",
    "- **Syntax**:  \n",
    "  ```python\n",
    "  df1.join(df2, on=[\"id\"], how=\"inner\")\n",
    "  ```\n",
    "- **Key Point**: Can lead to unexpected results if not carefully managed.\n",
    "\n",
    "---\n",
    "\n",
    "### 🏃 **Performance Considerations**\n",
    "- **Broadcast Join**: Use `.hint(\"broadcast\")` when one DataFrame is small enough to fit in memory to optimize join performance.  \n",
    "  ```python\n",
    "  df1.join(broadcast(df2), \"id\").show()\n",
    "  ```\n",
    "- **Partitioning**: Ensure data is partitioned optimally for large-scale joins.\n",
    "\n",
    "---\n",
    "\n",
    "### 📚 **Summary Table**\n",
    "\n",
    "| Join Type         | Description                                    | Missing Data Handling  | Performance Impact |\n",
    "|-------------------|------------------------------------------------|------------------------|--------------------|\n",
    "| **Inner Join**    | Matching rows from both DataFrames             | Excluded               | Efficient          |\n",
    "| **Left Join**     | All rows from left, matching from right         | Nulls on right         | Moderate           |\n",
    "| **Right Join**    | All rows from right, matching from left         | Nulls on left          | Moderate           |\n",
    "| **Full Join**     | All rows from both DataFrames                  | Nulls on both sides    | High               |\n",
    "| **Left Semi Join**| Matching rows from left only                   | Excluded               | Efficient          |\n",
    "| **Left Anti Join**| Non-matching rows from left only               | Excluded               | Efficient          |\n",
    "| **Cross Join**    | Cartesian product of both DataFrames           | None                   | Very High          |\n",
    "| **Self Join**     | Join DataFrame with itself                     | N/A                    | Varies             |\n",
    "| **Natural Join**  | Auto join on columns with same names           | Nulls on missing sides | Moderate           |\n",
    "\n",
    "---\n",
    "\n",
    "### 🎉 **Conclusion**  \n",
    "- Choose **`inner`** for intersection, **`left`**/**`right`** for preservation, and **`outer`** for complete combinations.  \n",
    "- Use **semi**/**anti** joins for filtering scenarios.  \n",
    "- Optimize performance with **broadcast joins** when dealing with small datasets.  \n",
    "- Always consider the **data size** and **join condition** for efficient joins in PySpark."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Column_operation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
